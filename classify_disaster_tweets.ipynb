{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import f_classif,SelectKBest\n",
    "import emoji\n",
    "from numpy import newaxis\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix,precision_score,recall_score,f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from nltk.tokenize import TweetTokenizer,word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import tensorflow\n",
    "import keras\n",
    "from keras.models import load_model\n",
    "import re\n",
    "stop_words=list(set(stopwords.words(\"english\")))\n",
    "stop_words=[word.lower() for word in stop_words]\n",
    "tokenizer=TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(sent):\n",
    "#     s=''\n",
    "    # sent=re.sub(r'#','<hashtag>',df['text'][i])\n",
    "    sent=emoji.demojize(sent)\n",
    "    sent=sent.lower()\n",
    "    sent=re.sub(r'[^a-zA-Z0-9]',' ',sent)\n",
    "    sent=re.sub(r'[\\s]+',' ',sent)\n",
    "    s=''\n",
    "    words=[word for word in word_tokenize(sent) if word not in stop_words]\n",
    "    for word in words:\n",
    "        if(len(s)==0):\n",
    "            s+=word\n",
    "        else:\n",
    "            s+=\" \"+word\n",
    "    sent=s\n",
    "    # sent=sent.lower()\n",
    "#     sent=re.sub(r'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&\\/\\/=]*)','<URL>',sent)  #Replace url by <URL>\n",
    "#     sent=re.sub(r'@RT\\[[a-zA-Z]+\\]|RT@\\[[a-zA-Z]]',' ',sent) \n",
    "#     sent=re.sub(r'@\\[[a-zA-Z]+\\]','<NAME>',sent)\n",
    "#     # sent=''.join(ch if ord(ch)<128 else '' for ch in sent)\n",
    "#     sent=re.sub(r'&','<AND>',sent)\n",
    "#     sent=re.sub(r',',' , ',sent)\n",
    "#     sent=re.sub(r'\\.',' . ',sent)\n",
    "#     sent=re.sub(r\"'\",\" ' \",sent)\n",
    "#     sent=re.sub(r'-',' - ',sent)\n",
    "#     sent=re.sub(r'[\\...]{3}',' ... ',sent)\n",
    "#     sent=re.sub(r'!',' ! ',sent)\n",
    "#     sent=re.sub(r'\\?',' ? ',sent)\n",
    "#     sent=re.sub(r'[0-9]','<number>',sent)\n",
    "#     sent=re.sub(r\"i'm\",\"i am\",sent)\n",
    "#     sent=re.sub(r\"they'are\",\"they are\",sent)\n",
    "#     sent=re.sub(r\"won't\",\"would not\",sent)\n",
    "#     sent=re.sub(r\"you're\",\"you are\",sent)\n",
    "#     sent=re.sub(r\"who're\",\"who are\",sent)\n",
    "#     sent=re.sub(r\"they've\",\"they have\",sent)\n",
    "#     sent=re.sub(r'[\\s]+',' ',sent)\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file):\n",
    "    df=pd.read_csv(file)\n",
    "    df=df[['text','target']]\n",
    "    sentences=[]\n",
    "    for sent in df['text']:\n",
    "        sentences.append(preprocessing(sent))\n",
    "    num_classes=len(df['target'].unique())\n",
    "    return df,sentences,num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_words_per_sample(text):\n",
    "    words=[len(set(tokenizer.tokenize(s))) for s in text]\n",
    "    return np.median(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_model(sentences):\n",
    "    s=len(sentences)\n",
    "    w=get_num_words_per_sample(sentences)\n",
    "    if(s//w<1500):\n",
    "        print(\"MLP is suitable\")\n",
    "    else:\n",
    "        print(\"Deep learning is suitable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(sentences,df):\n",
    "    X_train,X_test,Y_train,Y_test=train_test_split(sentences,df['target'],test_size=0.2,shuffle=True,random_state=42)\n",
    "    X_train,X_dev,Y_train,Y_dev=train_test_split(X_train,Y_train,test_size=0.2,shuffle=True,random_state=42)\n",
    "    return X_train,Y_train,X_dev,Y_dev,X_test,Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_label(train_labels,dev_labels,test_labels):\n",
    "    le=LabelEncoder()\n",
    "    le=le.fit(train_labels)\n",
    "    train_labels=le.transform(train_labels)\n",
    "    dev_labels=le.transform(dev_labels)\n",
    "    test_labels=le.transform(test_labels)\n",
    "    return train_labels,dev_labels,test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vectorize data\n",
    "ngram_range=(1,2)\n",
    "top_k=20000\n",
    "token_mode='word'\n",
    "min_document_frequency=2\n",
    "max_sequence_length=500\n",
    "def vectorize_ngram(train_data,train_labels,dev_data,test_data):\n",
    "    vectorizer=TfidfVectorizer(ngram_range=ngram_range,dtype='int32',strip_accents='unicode',decode_error='replace',analyzer=token_mode,min_df=min_document_frequency)\n",
    "    train_data=vectorizer.fit_transform(train_data)\n",
    "    dev_data=vectorizer.transform(dev_data)\n",
    "    test_data=vectorizer.transform(test_data)\n",
    "    selector=SelectKBest(f_classif,k=min(top_k,train_data.shape[1]))\n",
    "    selector.fit(train_data,train_labels)\n",
    "    train_data=selector.transform(train_data)\n",
    "    dev_data=selector.transform(dev_data)\n",
    "    test_data=selector.transform(test_data)\n",
    "    train_data=train_data.astype('float32')\n",
    "    dev_data=dev_data.astype('float32')\n",
    "    test_data=test_data.astype('float32')\n",
    "    return train_data,dev_data,test_data,selector,vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_non_numeric_to_numeric(X_train,Y_train,X_dev,Y_dev,X_test,Y_test):\n",
    "    Y_train,Y_dev,Y_test=encoding_label(Y_train,Y_dev,Y_test)\n",
    "    X_train,X_dev,X_test,selector,vectorizer=vectorize_ngram(X_train,Y_train,X_dev,X_test)\n",
    "    return X_train,Y_train,X_dev,Y_dev,X_test,Y_test,selector,vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_layer_units_activation(num_classes):\n",
    "    if(num_classes==2):\n",
    "        activation='sigmoid'\n",
    "        units=1\n",
    "    else:\n",
    "        activation='softmax'\n",
    "        units=num_classes\n",
    "    return activation,units\n",
    "def mlp_model(layers,units,dropout_rate,input_shape,num_classes):\n",
    "    output_activation,output_units=get_last_layer_units_activation(num_classes)\n",
    "    model=keras.models.Sequential()\n",
    "    model.add(keras.layers.Dropout(rate=dropout_rate,input_shape=input_shape))\n",
    "    for _ in range(layers-1):\n",
    "        model.add(keras.layers.Dense(units=units,activation='relu'))\n",
    "        model.add(keras.layers.Dropout(rate=dropout_rate))\n",
    "    model.add(keras.layers.Dense(units=output_units,activation=output_activation))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_text,train_label,dev_text,dev_label,layers=2,units=32,dropout_rate=0.2,epochs=150,learning_rate=0.00001,num_classes=2,batch_size=4):\n",
    "    model=mlp_model(layers=layers,units=units,dropout_rate=dropout_rate,input_shape=train_text.shape[1:],num_classes=num_classes)\n",
    "    if(num_classes==2):\n",
    "        loss='binary_crossentropy'\n",
    "    else:\n",
    "        loss='sparse_categorical_crossentropy'\n",
    "    optimizer=keras.optimizers.Adam(lr=learning_rate)\n",
    "    model.compile(optimizer=optimizer,loss=loss,metrics=['acc'])\n",
    "#     callbacks=[keras.callbacks.EarlyStopping(monitor='val_loss',patience=3)]\n",
    "    history=model.fit(train_text,train_label,epochs=epochs,validation_data=(dev_text,dev_label),batch_size=batch_size,verbose=2)\n",
    "    history = history.history\n",
    "    print('Validation accuracy: {acc}, loss: {loss}'.format(acc=history['val_acc'][-1], loss=history['val_loss'][-1]))\n",
    "    model.save('disaster_tweets.h5')\n",
    "    return history['val_acc'][-1], history['val_loss'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluating_algorithm(Y,Y_pred):\n",
    "    matrix=confusion_matrix(Y,Y_pred)\n",
    "    p=precision_score(Y,Y_pred)\n",
    "    r=recall_score(Y,Y_pred)\n",
    "    f1=f1_score(Y,Y_pred)\n",
    "    return matrix,p,r,f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_disaster_tweets(file):\n",
    "    df,sentences,no_of_classes=read_file(file)\n",
    "    choose_model(sentences)\n",
    "    X_train,Y_train,X_dev,Y_dev,X_test,Y_test=split_dataset(sentences,df)\n",
    "    X_train,Y_train,X_dev,Y_dev,X_test,Y_test,selector,vectorizer=convert_non_numeric_to_numeric(X_train,Y_train,X_dev,Y_dev,X_test,Y_test)\n",
    "    train_model(X_train,Y_train,X_dev,Y_dev)\n",
    "    model=load_model('disaster_tweets.h5')\n",
    "    model.evaluate(X_test,Y_test)\n",
    "    Y_pred=model.predict_classes(X_test)\n",
    "    matrix,p,r,f1=evaluating_algorithm(Y_test,Y_pred)\n",
    "    print(\"Confusion Matrix:-\\n\",matrix)\n",
    "    print(\"Precision:-\\n\",p)\n",
    "    print(\"Recall:-\\n\",r)\n",
    "    print(\"F1 score:-\\n\",f1)\n",
    "    return model,selector,vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model,selector,vectorizer=classify_disaster_tweets('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_unknown_dataset(file,vectorizer,selector,model):\n",
    "    df_test=pd.read_csv(file)\n",
    "    sentences=[]\n",
    "    for sent in df_test['text']:\n",
    "        sentences.append(preprocessing(sent))\n",
    "    sentences=vectorizer.transform(sentences)\n",
    "    sentences=selector.transform(sentences)\n",
    "    sentences=sentences.astype('float32')\n",
    "    test_result=model.predict_classes(sentences)\n",
    "    submission={\n",
    "    'text':[],\n",
    "    'target':[]\n",
    "    }\n",
    "    for i in range(0,len(df_test)):\n",
    "        submission['text'].append(df_test['text'][i])\n",
    "        submission['target'].append(test_result[i][0])\n",
    "    return submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission=test_unknown_dataset('test.csv',vectorizer,selector,model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
